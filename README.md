IMDB Reviews Checker

This project is designed to analyze movie reviews using machine learning. It fetches the IMDB dataset from Dropbox, processes it, and applies natural language processing techniques to classify reviews as positive or negative.

ğŸ“Œ Features

Fetches dataset from Dropbox automatically.

Preprocesses text data (removes stopwords, tokenizes, etc.).

Implements machine learning models for sentiment analysis.

Saves trained models for future use.

Provides performance plots for model evaluation.

ğŸ› ï¸ Setup & Installation

Follow these steps to set up and run the project:

1ï¸âƒ£ Clone the Repository

git clone https://github.com/Abuhurera-coder/IMDB-REVIEWS-CHECKER.git
cd IMDB-REVIEWS-CHECKER

2ï¸âƒ£ Install Dependencies

Make sure you have Python installed. Then, run:

pip install -r requirements.txt

This installs necessary packages like Pandas, NumPy, Scikit-learn, etc.

3ï¸âƒ£ Fetch IMDB Dataset from Dropbox

We are using Dropbox to store and retrieve the dataset. The dataset is specified in config.py:

DATASET_URL = "https://www.dropbox.com/scl/fi/e0htuwzj1yfdy4srn1mxd/IMDB-Dataset.csv?rlkey=iw3mf3xn16kqj81kg3ozrqsqn&st=8vgtki29&dl=1"

To download the dataset, run:

python download_dataset.py

This script fetches the dataset and saves it as IMDB Dataset.csv in your project folder.

ğŸ“‚ Project Structure

ğŸ“‚ IMDB-REVIEWS-CHECKER/
â”œâ”€â”€ ğŸ“„ main.py                 # Main script to train and evaluate the model
â”œâ”€â”€ ğŸ“„ config.py               # Configuration settings
â”œâ”€â”€ ğŸ“„ download_dataset.py     # Script to download dataset from Dropbox
â”œâ”€â”€ ğŸ“„ preprocess.py           # Text preprocessing functions
â”œâ”€â”€ ğŸ“„ train_model.py          # Model training script
â”œâ”€â”€ ğŸ“„ evaluate.py             # Model evaluation script
â”œâ”€â”€ ğŸ“‚ model_artifacts/        # Saved models and vectorizers
â”œâ”€â”€ ğŸ“‚ performance_plots/      # Plots for model evaluation
â”œâ”€â”€ ğŸ“„ requirements.txt        # Required dependencies
â”œâ”€â”€ ğŸ“„ README.md               # Documentation
â””â”€â”€ ğŸ“„ .gitignore              # Ignore unnecessary files

ğŸ“ Usage Instructions

4ï¸âƒ£ Preprocess the Data

Before training, we need to clean the dataset. Run:

python preprocess.py

This removes unwanted characters, tokenizes text, and prepares data for model training.

5ï¸âƒ£ Train the Model

Train the sentiment analysis model:

python train_model.py

This will:

Extract features from the text (TF-IDF vectorization)

Train a machine learning model

Save the trained model to model_artifacts/

6ï¸âƒ£ Evaluate the Model

Once trained, test the model:

python evaluate.py

This script:

Loads the trained model

Evaluates it on test data

Generates accuracy and confusion matrix plots in performance_plots/

7ï¸âƒ£ Predict New Reviews

To classify a new review, use:

python predict.py "This movie was fantastic!"

ğŸ’¾ Dropbox Integration

This project uses Dropbox to store the dataset. You can replace the dataset URL in config.py with your own Dropbox link:

DATASET_URL = "your-dropbox-file-link"

Ensure that:

The link ends with dl=1 to allow direct downloads.

You update download_dataset.py accordingly.

ğŸ“Š Performance Evaluation

Model accuracy and confusion matrix will be saved in performance_plots/.

You can modify evaluate.py to test different evaluation metrics.

ğŸš€ Next Steps

Improve text preprocessing.

Experiment with different ML models.

Deploy the model using Flask or FastAPI.

ğŸ› ï¸ Troubleshooting

Error: ModuleNotFoundError: No module named 'pandas'Solution: Install dependencies using pip install -r requirements.txt.

Error: Git push failed due to rejected updatesSolution: Run git pull origin main --rebase before pushing.

ğŸ‘¨â€ğŸ’» Contributing

Want to improve this project? Feel free to fork and submit a pull request!

ğŸ“œ License

This project is open-source under the MIT License.

ğŸ”— Connect with Me

GitHub: Abuhurera-coder

If you face any issue feel free to contact me.
